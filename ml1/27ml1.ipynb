{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"27ml1.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"425b290b3300794b0c51490054efb061","grade":false,"grade_id":"cell-d16f9036bd325625","locked":true,"schema_version":3,"solution":false,"task":false},"id":"EvIZE98OTHQe"},"source":["<div style=\"background:#222222; color:#ffffff; padding:20px\">\n","<h1 align=\"center\">Guided ML With The Iris Dataset</h1>\n","\n","<h2 align=\"center\" tyle=\"color:#01ff84\" >Learning type | Activity type | Objective |</h2>\n","<h2 align=\"center\">| Supervised | Multiclass classification | Identify a flower's class |</h2>\n","\n","\n","<div>"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"829109418410f732b20fa5260845eae2","grade":false,"grade_id":"cell-c273e6623fe661ab","locked":true,"schema_version":3,"solution":false,"task":false},"id":"KhPTNrtjTHQg"},"source":["Contents:\n","\n","1. Loading the data\n","2. Setting up supervised learning problem (selecting features)\n","3. Creating a first model\n","    - Creating train and test datasets\n","    - Normalizing train and test\n","    - Fitting and predicting\n","4. Evaluate the frist model predictions\n","5. Crossvalidation of the model\n","6. Creating an end to end ML pipeline\n","    - Train/Test Split\n","    - Normalize\n","    - Crossvalidations\n","    - Model\n","    - fitting and predicting"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"8d08b8142820842aefcd5c956a3206ac","grade":false,"grade_id":"cell-b9b45906f7798060","locked":true,"schema_version":3,"solution":false,"task":false},"id":"c3wAJZekTHQh"},"source":["## Instructions with NBGrader removed\n","\n","Complete the cells beginning with `# YOUR CODE HERE` and run the subsequent cells to check your code."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f6f9da48ae5f9afe18e5f66e254f2c8e","grade":false,"grade_id":"cell-ecfedf24107f7412","locked":true,"schema_version":3,"solution":false,"task":false},"id":"Aayfs81sTHQi"},"source":["\n","Contents:\n","1. Loading the data\n","2. Setting up supervised learning problem (selecting features)\n","3. Creating a first model\n","    - Creating train and test datasets\n","    - Normalizing train and test\n","    - Fitting and predicting\n","4. Evaluate the frist model predictions\n","5. Crossvalidation of the model\n","6. Creating an end to end ML pipeline\n","    - Train/Test Split\n","    - Normalize\n","    - Crossvalidations\n","    - Model\n","    - fitting and predicting"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d550a34573fd8f474e045823d568a379","grade":false,"grade_id":"cell-d265ce864af311cc","locked":true,"schema_version":3,"solution":false,"task":false},"id":"hGwC_RVbTHQi"},"source":["## About the dataset\n","\n","[Iris](https://archive.ics.uci.edu/ml/datasets/iris) is a well-known multiclass dataset. It contains 3 classes of flowers with 50 examples each. There are a total of 4 features for each flower.\n","\n","![](./classic-datasets/images/Iris-versicolor-21_1.jpg)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"87b95fe75781b483f205b07196ee8d7f","grade":false,"grade_id":"cell-970959de1583c6aa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"K7dCOBkATHQj"},"source":["## Package setups\n","\n","1. Run the following two cells to initalize the required libraries. "]},{"cell_type":"code","metadata":{"id":"vRtKjJ6nTHQj","outputId":"db179ca8-0349-4e12-86bf-28babbc6ee9b"},"source":["#to debug package errors\n","import sys\n","sys.path\n","sys.executable"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'C:\\\\Users\\\\Dilan\\\\anaconda3\\\\envs\\\\Strive_School_AI_March_2021\\\\python.exe'"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"Owkkoc0pTHQk"},"source":["# Import needed packages\n","# You may add or remove packages should you need them\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import datasets\n","from sklearn import model_selection\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import preprocessing\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, cross_validate\n","from sklearn.pipeline import make_pipeline\n","\n","# Display plots inline and change plot resolution to retina\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","# Set Seaborn aesthetic parameters to defaults\n","sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bh0vM0bXTHQk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a339b0712c560653440a7d36ab5e8bc8","grade":false,"grade_id":"cell-c1bf14dba6281d8f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"joMJEMNhTHQl"},"source":["## Step 1: Loading the data\n","\n","1. Load the iris dataset using ```datasets.load_iris()```\n","2. Investigate the data structure with ```.keys()```\n","3. Construct a dataframe from the dataset\n","4. Create a 'target' and a 'class' column that contains the target names and values\n","5. Display a random sample of the dataframe "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"4779ca50002373e235c91b4bc239fd44","grade":false,"grade_id":"cell-c92cca7755a29a2c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"j6NbHuz0THQl"},"source":["def load_data():\n","    #load the dataset\n","    #return the dataset\n","    ds = datasets.load_iris()\n","    return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2dff5e326550ba31c1bb6ba5d90e251e","grade":true,"grade_id":"cell-6955aa3f0eb9b484","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"JkBhnvTfTHQm"},"source":["assert load_data()['data'].shape == (150,4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJH1SoX_THQm","outputId":"db1a86be-686b-42e6-83aa-b106eb8ba381"},"source":["print(load_data().keys())\n","dataset = load_data()\n","load_data().feature_names\n","print(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n","{'data': array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'C:\\\\Users\\\\Dilan\\\\anaconda3\\\\envs\\\\Strive_School_AI_March_2021\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0854adf2bd82feb9c7409fcff160ea6a","grade":false,"grade_id":"cell-054f7e75dfc37e15","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Qz_3DntmTHQn","outputId":"9792090d-936a-4bd6-dfe1-9637d5f1ae42"},"source":["\n","def dataset_to_pandas():\n","    #put the dataset into a pandas DF using the feature names as columnsç\n","    #rename the column name so the dont include the '(cm)'\n","    #add 2 columns one with the target and another with the target_names\n","    \n","    # YOUR CODE HERE\n","    df = pd.DataFrame(dataset['data'],columns=['sepal length','sepal width','petal length','petal width'])\n","    df['target'] = pd.DataFrame(dataset['target'])\n","    df['class'] = df['target'].replace([0,1,2],dataset['target_names'])\n","    return df\n","print(dataset_to_pandas())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["     sepal length  sepal width  petal length  petal width  target      class\n","0             5.1          3.5           1.4          0.2       0     setosa\n","1             4.9          3.0           1.4          0.2       0     setosa\n","2             4.7          3.2           1.3          0.2       0     setosa\n","3             4.6          3.1           1.5          0.2       0     setosa\n","4             5.0          3.6           1.4          0.2       0     setosa\n","..            ...          ...           ...          ...     ...        ...\n","145           6.7          3.0           5.2          2.3       2  virginica\n","146           6.3          2.5           5.0          1.9       2  virginica\n","147           6.5          3.0           5.2          2.0       2  virginica\n","148           6.2          3.4           5.4          2.3       2  virginica\n","149           5.9          3.0           5.1          1.8       2  virginica\n","\n","[150 rows x 6 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e803679e20976baff81ad4577a79bfec","grade":true,"grade_id":"cell-b18a01895567bcda","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"jX0IgTpATHQo"},"source":["df = dataset_to_pandas()\n","assert df['sepal length'].shape == (150,)\n","assert df['sepal width'].shape == (150,)\n","assert df['petal length'].shape == (150,)\n","assert df['petal width'].shape == (150,)\n","assert df['target'].shape == (150,)\n","assert df['class'].shape == (150,)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoLhobTqTHQp"},"source":["### Question\n","Find the X and y values we're looking for. Notice that y is categorical and thus, we could **one-hot encode it** if we are looking at **class** or we can just pick **target**. In order to one hot encode we have  to re-shape `y` it using the **.get_dummies** function. \n","\n","## For the purpose of this exercise, do not use hot encoding, go only for target but think about if you have to drop it somewhere or not..."]},{"cell_type":"code","metadata":{"id":"oxCKRL5ITHQp","outputId":"b0087e0d-99f3-45a2-bbc1-6a69eef6a6d4"},"source":["df_iris = dataset_to_pandas()\n","type(df_iris)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pandas.core.frame.DataFrame"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3a8ff0960c61f4785cbc8f88fd683036","grade":false,"grade_id":"cell-fd162bfaf4394e79","locked":false,"schema_version":3,"solution":true,"task":false},"id":"FMYzdIAGTHQq","outputId":"2ec44076-89c3-434c-88c8-bb905c340f99"},"source":["def ohe():\n","    dummy = pd.get_dummies(df_iris, columns=[\"target\"])\n","\n","    return dummy\n","print(ohe().head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   sepal length  sepal width  petal length  petal width   class  target_0  \\\n","0           5.1          3.5           1.4          0.2  setosa         1   \n","1           4.9          3.0           1.4          0.2  setosa         1   \n","2           4.7          3.2           1.3          0.2  setosa         1   \n","3           4.6          3.1           1.5          0.2  setosa         1   \n","4           5.0          3.6           1.4          0.2  setosa         1   \n","\n","   target_1  target_2  \n","0         0         0  \n","1         0         0  \n","2         0         0  \n","3         0         0  \n","4         0         0  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fd1e5d3933256fa113642e421db19875","grade":true,"grade_id":"cell-423f24383553b71a","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"igK9x-k1THQr"},"source":["ohe_data = ohe()\n","\n","assert ohe_data.shape == (150,8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyJGucl3THQs"},"source":["## Step 2: Setting up supervised learning problem (selecting features)\n","\n","Feature selection is an essential step in improving a model's perfromance. In the first version of the model we will use the **'sepal length'** and **'sepal width'** as predicting features. Later we will see the effect of adding additional features.\n","\n","1. Assign the values of the 'target' to Y as a numpy array\n","2. Assign the remaining feature values to X as a numpy array\n","3. Check the shape of X and Y. Check the first few values.\n","    - Can we confirm our X and Y are created correctly?"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"56b10067459820c4e0a12ebf6efe8b5b","grade":false,"grade_id":"cell-c0cf28fd18447a0b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"-KLBWmlcTHQs","outputId":"b6a90b8a-960c-4a04-df5a-0e8c8e2cc1a5"},"source":["def target_to_numpy():\n","    y = df_iris['target'].values\n","    return y\n","def data_to_numpy(): \n","    x = df_iris[['sepal length', 'sepal width']].values\n","    print(x)\n","    return x\n","data_to_numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[5.1 3.5]\n"," [4.9 3. ]\n"," [4.7 3.2]\n"," [4.6 3.1]\n"," [5.  3.6]\n"," [5.4 3.9]\n"," [4.6 3.4]\n"," [5.  3.4]\n"," [4.4 2.9]\n"," [4.9 3.1]\n"," [5.4 3.7]\n"," [4.8 3.4]\n"," [4.8 3. ]\n"," [4.3 3. ]\n"," [5.8 4. ]\n"," [5.7 4.4]\n"," [5.4 3.9]\n"," [5.1 3.5]\n"," [5.7 3.8]\n"," [5.1 3.8]\n"," [5.4 3.4]\n"," [5.1 3.7]\n"," [4.6 3.6]\n"," [5.1 3.3]\n"," [4.8 3.4]\n"," [5.  3. ]\n"," [5.  3.4]\n"," [5.2 3.5]\n"," [5.2 3.4]\n"," [4.7 3.2]\n"," [4.8 3.1]\n"," [5.4 3.4]\n"," [5.2 4.1]\n"," [5.5 4.2]\n"," [4.9 3.1]\n"," [5.  3.2]\n"," [5.5 3.5]\n"," [4.9 3.6]\n"," [4.4 3. ]\n"," [5.1 3.4]\n"," [5.  3.5]\n"," [4.5 2.3]\n"," [4.4 3.2]\n"," [5.  3.5]\n"," [5.1 3.8]\n"," [4.8 3. ]\n"," [5.1 3.8]\n"," [4.6 3.2]\n"," [5.3 3.7]\n"," [5.  3.3]\n"," [7.  3.2]\n"," [6.4 3.2]\n"," [6.9 3.1]\n"," [5.5 2.3]\n"," [6.5 2.8]\n"," [5.7 2.8]\n"," [6.3 3.3]\n"," [4.9 2.4]\n"," [6.6 2.9]\n"," [5.2 2.7]\n"," [5.  2. ]\n"," [5.9 3. ]\n"," [6.  2.2]\n"," [6.1 2.9]\n"," [5.6 2.9]\n"," [6.7 3.1]\n"," [5.6 3. ]\n"," [5.8 2.7]\n"," [6.2 2.2]\n"," [5.6 2.5]\n"," [5.9 3.2]\n"," [6.1 2.8]\n"," [6.3 2.5]\n"," [6.1 2.8]\n"," [6.4 2.9]\n"," [6.6 3. ]\n"," [6.8 2.8]\n"," [6.7 3. ]\n"," [6.  2.9]\n"," [5.7 2.6]\n"," [5.5 2.4]\n"," [5.5 2.4]\n"," [5.8 2.7]\n"," [6.  2.7]\n"," [5.4 3. ]\n"," [6.  3.4]\n"," [6.7 3.1]\n"," [6.3 2.3]\n"," [5.6 3. ]\n"," [5.5 2.5]\n"," [5.5 2.6]\n"," [6.1 3. ]\n"," [5.8 2.6]\n"," [5.  2.3]\n"," [5.6 2.7]\n"," [5.7 3. ]\n"," [5.7 2.9]\n"," [6.2 2.9]\n"," [5.1 2.5]\n"," [5.7 2.8]\n"," [6.3 3.3]\n"," [5.8 2.7]\n"," [7.1 3. ]\n"," [6.3 2.9]\n"," [6.5 3. ]\n"," [7.6 3. ]\n"," [4.9 2.5]\n"," [7.3 2.9]\n"," [6.7 2.5]\n"," [7.2 3.6]\n"," [6.5 3.2]\n"," [6.4 2.7]\n"," [6.8 3. ]\n"," [5.7 2.5]\n"," [5.8 2.8]\n"," [6.4 3.2]\n"," [6.5 3. ]\n"," [7.7 3.8]\n"," [7.7 2.6]\n"," [6.  2.2]\n"," [6.9 3.2]\n"," [5.6 2.8]\n"," [7.7 2.8]\n"," [6.3 2.7]\n"," [6.7 3.3]\n"," [7.2 3.2]\n"," [6.2 2.8]\n"," [6.1 3. ]\n"," [6.4 2.8]\n"," [7.2 3. ]\n"," [7.4 2.8]\n"," [7.9 3.8]\n"," [6.4 2.8]\n"," [6.3 2.8]\n"," [6.1 2.6]\n"," [7.7 3. ]\n"," [6.3 3.4]\n"," [6.4 3.1]\n"," [6.  3. ]\n"," [6.9 3.1]\n"," [6.7 3.1]\n"," [6.9 3.1]\n"," [5.8 2.7]\n"," [6.8 3.2]\n"," [6.7 3.3]\n"," [6.7 3. ]\n"," [6.3 2.5]\n"," [6.5 3. ]\n"," [6.2 3.4]\n"," [5.9 3. ]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 3.5],\n","       [4.9, 3. ],\n","       [4.7, 3.2],\n","       [4.6, 3.1],\n","       [5. , 3.6],\n","       [5.4, 3.9],\n","       [4.6, 3.4],\n","       [5. , 3.4],\n","       [4.4, 2.9],\n","       [4.9, 3.1],\n","       [5.4, 3.7],\n","       [4.8, 3.4],\n","       [4.8, 3. ],\n","       [4.3, 3. ],\n","       [5.8, 4. ],\n","       [5.7, 4.4],\n","       [5.4, 3.9],\n","       [5.1, 3.5],\n","       [5.7, 3.8],\n","       [5.1, 3.8],\n","       [5.4, 3.4],\n","       [5.1, 3.7],\n","       [4.6, 3.6],\n","       [5.1, 3.3],\n","       [4.8, 3.4],\n","       [5. , 3. ],\n","       [5. , 3.4],\n","       [5.2, 3.5],\n","       [5.2, 3.4],\n","       [4.7, 3.2],\n","       [4.8, 3.1],\n","       [5.4, 3.4],\n","       [5.2, 4.1],\n","       [5.5, 4.2],\n","       [4.9, 3.1],\n","       [5. , 3.2],\n","       [5.5, 3.5],\n","       [4.9, 3.6],\n","       [4.4, 3. ],\n","       [5.1, 3.4],\n","       [5. , 3.5],\n","       [4.5, 2.3],\n","       [4.4, 3.2],\n","       [5. , 3.5],\n","       [5.1, 3.8],\n","       [4.8, 3. ],\n","       [5.1, 3.8],\n","       [4.6, 3.2],\n","       [5.3, 3.7],\n","       [5. , 3.3],\n","       [7. , 3.2],\n","       [6.4, 3.2],\n","       [6.9, 3.1],\n","       [5.5, 2.3],\n","       [6.5, 2.8],\n","       [5.7, 2.8],\n","       [6.3, 3.3],\n","       [4.9, 2.4],\n","       [6.6, 2.9],\n","       [5.2, 2.7],\n","       [5. , 2. ],\n","       [5.9, 3. ],\n","       [6. , 2.2],\n","       [6.1, 2.9],\n","       [5.6, 2.9],\n","       [6.7, 3.1],\n","       [5.6, 3. ],\n","       [5.8, 2.7],\n","       [6.2, 2.2],\n","       [5.6, 2.5],\n","       [5.9, 3.2],\n","       [6.1, 2.8],\n","       [6.3, 2.5],\n","       [6.1, 2.8],\n","       [6.4, 2.9],\n","       [6.6, 3. ],\n","       [6.8, 2.8],\n","       [6.7, 3. ],\n","       [6. , 2.9],\n","       [5.7, 2.6],\n","       [5.5, 2.4],\n","       [5.5, 2.4],\n","       [5.8, 2.7],\n","       [6. , 2.7],\n","       [5.4, 3. ],\n","       [6. , 3.4],\n","       [6.7, 3.1],\n","       [6.3, 2.3],\n","       [5.6, 3. ],\n","       [5.5, 2.5],\n","       [5.5, 2.6],\n","       [6.1, 3. ],\n","       [5.8, 2.6],\n","       [5. , 2.3],\n","       [5.6, 2.7],\n","       [5.7, 3. ],\n","       [5.7, 2.9],\n","       [6.2, 2.9],\n","       [5.1, 2.5],\n","       [5.7, 2.8],\n","       [6.3, 3.3],\n","       [5.8, 2.7],\n","       [7.1, 3. ],\n","       [6.3, 2.9],\n","       [6.5, 3. ],\n","       [7.6, 3. ],\n","       [4.9, 2.5],\n","       [7.3, 2.9],\n","       [6.7, 2.5],\n","       [7.2, 3.6],\n","       [6.5, 3.2],\n","       [6.4, 2.7],\n","       [6.8, 3. ],\n","       [5.7, 2.5],\n","       [5.8, 2.8],\n","       [6.4, 3.2],\n","       [6.5, 3. ],\n","       [7.7, 3.8],\n","       [7.7, 2.6],\n","       [6. , 2.2],\n","       [6.9, 3.2],\n","       [5.6, 2.8],\n","       [7.7, 2.8],\n","       [6.3, 2.7],\n","       [6.7, 3.3],\n","       [7.2, 3.2],\n","       [6.2, 2.8],\n","       [6.1, 3. ],\n","       [6.4, 2.8],\n","       [7.2, 3. ],\n","       [7.4, 2.8],\n","       [7.9, 3.8],\n","       [6.4, 2.8],\n","       [6.3, 2.8],\n","       [6.1, 2.6],\n","       [7.7, 3. ],\n","       [6.3, 3.4],\n","       [6.4, 3.1],\n","       [6. , 3. ],\n","       [6.9, 3.1],\n","       [6.7, 3.1],\n","       [6.9, 3.1],\n","       [5.8, 2.7],\n","       [6.8, 3.2],\n","       [6.7, 3.3],\n","       [6.7, 3. ],\n","       [6.3, 2.5],\n","       [6.5, 3. ],\n","       [6.2, 3.4],\n","       [5.9, 3. ]])"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"97b1e0d84441ff74af70eef303cb09da","grade":true,"grade_id":"cell-aaf03dfa38041beb","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"UoeOcG_2THQt","outputId":"6f7292a0-692d-4ad9-e0c9-f8c506b61f5f"},"source":["Y = target_to_numpy()\n","X = data_to_numpy()\n","assert isinstance(Y, np.ndarray)\n","assert isinstance(X, np.ndarray)\n","assert X.shape == (150,2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[5.1 3.5]\n"," [4.9 3. ]\n"," [4.7 3.2]\n"," [4.6 3.1]\n"," [5.  3.6]\n"," [5.4 3.9]\n"," [4.6 3.4]\n"," [5.  3.4]\n"," [4.4 2.9]\n"," [4.9 3.1]\n"," [5.4 3.7]\n"," [4.8 3.4]\n"," [4.8 3. ]\n"," [4.3 3. ]\n"," [5.8 4. ]\n"," [5.7 4.4]\n"," [5.4 3.9]\n"," [5.1 3.5]\n"," [5.7 3.8]\n"," [5.1 3.8]\n"," [5.4 3.4]\n"," [5.1 3.7]\n"," [4.6 3.6]\n"," [5.1 3.3]\n"," [4.8 3.4]\n"," [5.  3. ]\n"," [5.  3.4]\n"," [5.2 3.5]\n"," [5.2 3.4]\n"," [4.7 3.2]\n"," [4.8 3.1]\n"," [5.4 3.4]\n"," [5.2 4.1]\n"," [5.5 4.2]\n"," [4.9 3.1]\n"," [5.  3.2]\n"," [5.5 3.5]\n"," [4.9 3.6]\n"," [4.4 3. ]\n"," [5.1 3.4]\n"," [5.  3.5]\n"," [4.5 2.3]\n"," [4.4 3.2]\n"," [5.  3.5]\n"," [5.1 3.8]\n"," [4.8 3. ]\n"," [5.1 3.8]\n"," [4.6 3.2]\n"," [5.3 3.7]\n"," [5.  3.3]\n"," [7.  3.2]\n"," [6.4 3.2]\n"," [6.9 3.1]\n"," [5.5 2.3]\n"," [6.5 2.8]\n"," [5.7 2.8]\n"," [6.3 3.3]\n"," [4.9 2.4]\n"," [6.6 2.9]\n"," [5.2 2.7]\n"," [5.  2. ]\n"," [5.9 3. ]\n"," [6.  2.2]\n"," [6.1 2.9]\n"," [5.6 2.9]\n"," [6.7 3.1]\n"," [5.6 3. ]\n"," [5.8 2.7]\n"," [6.2 2.2]\n"," [5.6 2.5]\n"," [5.9 3.2]\n"," [6.1 2.8]\n"," [6.3 2.5]\n"," [6.1 2.8]\n"," [6.4 2.9]\n"," [6.6 3. ]\n"," [6.8 2.8]\n"," [6.7 3. ]\n"," [6.  2.9]\n"," [5.7 2.6]\n"," [5.5 2.4]\n"," [5.5 2.4]\n"," [5.8 2.7]\n"," [6.  2.7]\n"," [5.4 3. ]\n"," [6.  3.4]\n"," [6.7 3.1]\n"," [6.3 2.3]\n"," [5.6 3. ]\n"," [5.5 2.5]\n"," [5.5 2.6]\n"," [6.1 3. ]\n"," [5.8 2.6]\n"," [5.  2.3]\n"," [5.6 2.7]\n"," [5.7 3. ]\n"," [5.7 2.9]\n"," [6.2 2.9]\n"," [5.1 2.5]\n"," [5.7 2.8]\n"," [6.3 3.3]\n"," [5.8 2.7]\n"," [7.1 3. ]\n"," [6.3 2.9]\n"," [6.5 3. ]\n"," [7.6 3. ]\n"," [4.9 2.5]\n"," [7.3 2.9]\n"," [6.7 2.5]\n"," [7.2 3.6]\n"," [6.5 3.2]\n"," [6.4 2.7]\n"," [6.8 3. ]\n"," [5.7 2.5]\n"," [5.8 2.8]\n"," [6.4 3.2]\n"," [6.5 3. ]\n"," [7.7 3.8]\n"," [7.7 2.6]\n"," [6.  2.2]\n"," [6.9 3.2]\n"," [5.6 2.8]\n"," [7.7 2.8]\n"," [6.3 2.7]\n"," [6.7 3.3]\n"," [7.2 3.2]\n"," [6.2 2.8]\n"," [6.1 3. ]\n"," [6.4 2.8]\n"," [7.2 3. ]\n"," [7.4 2.8]\n"," [7.9 3.8]\n"," [6.4 2.8]\n"," [6.3 2.8]\n"," [6.1 2.6]\n"," [7.7 3. ]\n"," [6.3 3.4]\n"," [6.4 3.1]\n"," [6.  3. ]\n"," [6.9 3.1]\n"," [6.7 3.1]\n"," [6.9 3.1]\n"," [5.8 2.7]\n"," [6.8 3.2]\n"," [6.7 3.3]\n"," [6.7 3. ]\n"," [6.3 2.5]\n"," [6.5 3. ]\n"," [6.2 3.4]\n"," [5.9 3. ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h7y4ecV-THQu"},"source":["#your code here\n","#X = df_iris[['sepal length', 'sepal width']].values\n","#print(X.shape)\n","#X[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUe5-B5vTHQu"},"source":["## Step 3: Creating the first model\n","\n","In lecture we learned about creating a train and test datasets, normalizing, and fitting a model. In this step we will see how to build a simple version of this.\n","\n","We have to be careful when constructing our train and test datasets. First, when we create train and test datasets we have to be careful that we always have the same datapoints in each set. Otherwise our results won't be reproduceable or we might introduce a bias into our model.\n","\n","We also need to be attentive to when we normalize the data. What would be the effect of normalizing the data (i.e. with StandardScaler to a range between 0 - 1) before we create our train and test sets? Effectively we would use information in the test set to structure the values in the training set and vice versa. Therefore normalizing train and test independently is the preferred method.\n","\n","1. Create X_train, X_test, Y_train, Y_test using ```train_test_split()``` with an 80/20 train/test split. Look in the SKLearn documentation to understand how the function works.\n","    - Inspect the first few rows of X_train.\n","    - Run the cell a few times. Do the first few rows change?\n","    - What option can we use in ```train_test_split()``` to stop this from happening?\n","2. Normalize the train and test datasets with ```StandardScaler```\n","    - We can fit the transform with ```.fit()``` and ```.transform()``` to apply it. Look in the documentation for an esample of how to do this.\n","    - Does it make sense to normalize Y_train and Y_test?\n","3. Initalize a ```LogisticRegression()``` model and use the ```.fit()``` method to initalize the first model.\n","    - We will pass the X_train and Y_train variables to the ```.fit()``` method.\n","    - Once the model is fit, use the ```.predict()``` with the X_test and save the output as predictions."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a1e55823f8c49a22a28af13f984d10ba","grade":false,"grade_id":"cell-586cc3c4a3d849fa","locked":false,"schema_version":3,"solution":true,"task":false},"id":"7uUZM8bATHQv","outputId":"0ee160f4-dcf9-45e5-8b96-2a4c17462205"},"source":["#split train and test data 80/20\n","#your code here\n","X_train, X_test, Y_train, Y_test = 0,0,0,0\n","\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n","\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)\n","X_train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(120, 2)\n","(120,)\n","(30, 2)\n","(30,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[6.4, 3.1],\n","       [5.4, 3. ],\n","       [5.2, 3.5],\n","       [6.1, 3. ],\n","       [6.4, 2.8],\n","       [5.2, 2.7],\n","       [5.7, 3.8],\n","       [6. , 2.7],\n","       [5.9, 3. ],\n","       [5.8, 2.6],\n","       [6.8, 3. ],\n","       [4.7, 3.2],\n","       [6.9, 3.1],\n","       [5. , 3.5],\n","       [5.4, 3.7],\n","       [5. , 2. ],\n","       [6.5, 3. ],\n","       [6.7, 3.3],\n","       [6. , 2.2],\n","       [6.7, 2.5],\n","       [5.6, 2.5],\n","       [7.7, 3. ],\n","       [6.3, 3.3],\n","       [5.5, 2.4],\n","       [6.3, 2.7],\n","       [6.3, 2.8],\n","       [4.9, 2.5],\n","       [6.3, 2.5],\n","       [7. , 3.2],\n","       [6.5, 3. ],\n","       [6. , 3.4],\n","       [4.8, 3.1],\n","       [5.8, 2.7],\n","       [5.6, 2.7],\n","       [5.6, 2.9],\n","       [5.5, 2.5],\n","       [6.1, 3. ],\n","       [7.2, 3.2],\n","       [5.3, 3.7],\n","       [4.3, 3. ],\n","       [6.4, 2.7],\n","       [5.7, 3. ],\n","       [5.4, 3.4],\n","       [5.7, 4.4],\n","       [6.9, 3.1],\n","       [4.6, 3.1],\n","       [5.9, 3. ],\n","       [5.1, 2.5],\n","       [4.6, 3.4],\n","       [6.2, 2.2],\n","       [7.2, 3.6],\n","       [5.7, 2.9],\n","       [4.8, 3. ],\n","       [7.1, 3. ],\n","       [6.9, 3.2],\n","       [6.5, 3. ],\n","       [6.4, 2.8],\n","       [5.1, 3.8],\n","       [4.8, 3.4],\n","       [6.5, 3.2],\n","       [6.7, 3.3],\n","       [4.5, 2.3],\n","       [6.2, 3.4],\n","       [4.9, 3. ],\n","       [5.7, 2.5],\n","       [6.9, 3.1],\n","       [4.4, 3.2],\n","       [5. , 3.6],\n","       [7.2, 3. ],\n","       [5.1, 3.5],\n","       [4.4, 3. ],\n","       [5.4, 3.9],\n","       [5.5, 2.3],\n","       [6.8, 3.2],\n","       [7.6, 3. ],\n","       [5.1, 3.5],\n","       [4.9, 3.1],\n","       [5.2, 3.4],\n","       [5.7, 2.8],\n","       [6.6, 3. ],\n","       [5. , 3.2],\n","       [5.1, 3.3],\n","       [6.4, 2.9],\n","       [5.4, 3.4],\n","       [7.7, 2.6],\n","       [4.9, 2.4],\n","       [7.9, 3.8],\n","       [6.7, 3.1],\n","       [5.2, 4.1],\n","       [6. , 3. ],\n","       [5.8, 4. ],\n","       [7.7, 2.8],\n","       [5.1, 3.8],\n","       [4.7, 3.2],\n","       [7.4, 2.8],\n","       [5. , 3.3],\n","       [6.3, 3.4],\n","       [5.7, 2.8],\n","       [5.8, 2.7],\n","       [5.7, 2.6],\n","       [6.4, 3.2],\n","       [6.7, 3. ],\n","       [6.3, 2.5],\n","       [6.7, 3. ],\n","       [5. , 3. ],\n","       [5.5, 2.4],\n","       [6.7, 3.1],\n","       [5.8, 2.7],\n","       [5.1, 3.4],\n","       [6.6, 2.9],\n","       [5.6, 3. ],\n","       [5.9, 3.2],\n","       [6.3, 2.3],\n","       [5.5, 3.5],\n","       [5.1, 3.7],\n","       [4.9, 3.1],\n","       [6.3, 2.9],\n","       [5.8, 2.7],\n","       [7.7, 3.8],\n","       [4.6, 3.2]])"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"79e08b4d1d8ba96ace800f9f7544289a","grade":true,"grade_id":"cell-4415454a211f5895","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"Jc6lKUy8THQw"},"source":["assert X_train.shape == (120,2)\n","assert Y_train.shape == (120,)\n","assert X_test.shape  == (30,2)\n","assert Y_test.shape  == (30,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"cae2c49fb6dd8edd9da705b91681b634","grade":false,"grade_id":"cell-d94703390548f2df","locked":false,"schema_version":3,"solution":true,"task":false},"id":"cFoMog1lTHQz","outputId":"12176e58-9968-484f-a818-59b04d54ee61"},"source":["#normalize the dataset\n","#your code here\n","scaler = preprocessing.StandardScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","X_train[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.61303014,  0.10850105],\n","       [-0.56776627, -0.12400121],\n","       [-0.80392556,  1.03851009],\n","       [ 0.25879121, -0.12400121],\n","       [ 0.61303014, -0.58900572]])"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"94c70e4e0a35aabf275dcfc54032c506","grade":true,"grade_id":"cell-fd525045d90798a3","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"03KoZyrQTHQz"},"source":["assert np.amin(X_train) >= -2.5\n","assert np.amax(X_train) <= 3.2\n","assert np.amin(X_test) >= -2\n","assert np.amin(X_test) <= 2.75"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"38fcab794674e83ebb32a2828a429d4b","grade":false,"grade_id":"cell-a91abe39c7261df3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"QxOnnHvVTHQ0","outputId":"74d39677-cc9b-4236-9f20-99cba10119cd"},"source":["#initalize and fit with Logistic Regression\n","predictions = 0\n","#your code here\n","clf = LogisticRegression(solver='lbfgs',multi_class='multinomial').fit(X_train, Y_train)\n","predictions = clf.predict(X_test)\n","#initalize the logistic regressor\n","#make predictions\n","# YOUR CODE HERE\n","print(predictions)\n","predictions.shape\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 1 0 2 0 2 0 2 2 1 1 2 1 2 1 0 1 1 0 0 1 1 0 0 2 0 0 2 1 0]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(30,)"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"47ce21dba76593cfaa123c1ab6e3faae","grade":true,"grade_id":"cell-61e5bc16122b7220","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"-oxcPW2eTHQ1"},"source":["assert predictions.shape == (30,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0cf62aaed986a743938e4194ee854464","grade":false,"grade_id":"cell-ad53c7b7761b7c33","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5rPfIDveTHQ2"},"source":["## Step 4: Evaluate the frist model's predictions\n","\n","We will learn more about how to evaluate the performance of a classifier in later lessons. For now we will use % accuracy as our metric. It is important to know that this metric only helps us understand the specific performance of our model and not, for example, where we can improve it, or where it already perfoms well.\n","\n","1. Use ```.score()``` to evaluate the performance of our first model."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"eee957f8a23e8bf11a08b81ca09b59de","grade":false,"grade_id":"cell-a19f9db86d767c57","locked":false,"schema_version":3,"solution":true,"task":false},"id":"PuUF17ewTHQ2","outputId":"0f3c77d0-18c4-4454-88f7-cb8eaa17ed2e"},"source":["score = 0\n","#evaluating the performace of our first model\n","#your code here\n","# YOUR CODE HERE\n","score = clf.score(X_test,Y_test)\n","print(score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7333333333333333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b0ffe44569c3d8cfd1b662cb1eee741a","grade":true,"grade_id":"cell-300a4e426c8fcfa1","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"ve9AR7GaTHQ3"},"source":["assert score >=0.7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f5c847b79b21710ec7f481b2e5641562","grade":true,"grade_id":"cell-f7dbd3aa1419397f","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"76vVLtHMTHQ3"},"source":["assert score >=0.72"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ba1d8e7eeb90894ad0d9ad61f191cacc","grade":true,"grade_id":"cell-3b6954a5b2f244cd","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"vmyHWMeGTHQ4"},"source":["assert score >=0.73"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c79bec6a741d60a82f6bcbaf802ea63a","grade":false,"grade_id":"cell-5606fc0823aee705","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fLezgL-iTHQ4"},"source":["## Step 5: Crossvalidation of the model\n","Our first model achived ~90% accruacy. This is quite good. How do we know it is reproducable? If we run the model again and our performance is 85% which is correct? And what about improving our model? Can you think of one thing we can do to potentially improve the model?\n","\n","#### Crossvalidation\n","Corssvalidation is when we create multiple X and Y datasets. On each dataset we train and fit the model. We then average the results and return a 'crossvalidated' accruacy.\n","\n","1. Initalize a new version of the model you trained above with the same paramters.\n","2. Use ```cross_validate()``` to run the model with 5 crossvalidation folds. "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a94fe1023a3040a963c76b37ca85c8cb","grade":false,"grade_id":"cell-49d1242d4949806f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"j692ad3OTHQ5","outputId":"44756182-3a94-4ffd-c843-66cd279d911a"},"source":["#model with cross validation\n","#your code here\n","#cross validate the training set\n","clf_cv = 0\n","CV = 0\n","# YOUR CODE HERE\n","#cv = cross_validate(log_reg, X_train, Y_train)\n","log_reg = LogisticRegression()\n","cv = cross_validate(log_reg,X_train, Y_train, cv=5)\n","\n","def print_scores(cv):\n","    #print out cross validation scores\n","    [print('Crossvalidation fold: {}  Accruacy: {}'.format(n, score)) for n, score in enumerate(cv['test_score'])]\n","    #print out the mean of the cross validation\n","    print('Mean train cross validation score {}'.format(cv['test_score'].mean()))\n","    \n","print_scores(cv)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Crossvalidation fold: 0  Accruacy: 0.9166666666666666\n","Crossvalidation fold: 1  Accruacy: 0.7083333333333334\n","Crossvalidation fold: 2  Accruacy: 0.8333333333333334\n","Crossvalidation fold: 3  Accruacy: 0.7916666666666666\n","Crossvalidation fold: 4  Accruacy: 0.7916666666666666\n","Mean train cross validation score 0.8083333333333333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f654258178a1653e2150e77099a2517c","grade":true,"grade_id":"cell-cbdb686462858d91","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"id":"zwdEqtRZTHQ5"},"source":["assert len(cv['test_score']) == 5\n","assert max(cv['test_score']) >= 0.85\n","assert min(cv['test_score']) >= 0.69\n","assert cv['test_score'].mean() >= 0.77"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3715a10f14e6e81bfbdd7fff9c516566","grade":false,"grade_id":"cell-979159550aa72926","locked":true,"schema_version":3,"solution":false,"task":false},"id":"mzZt9xZTTHQ6"},"source":["## Step 6: Creating an end to end ML pipeline\n","Congraulations you've trained, crossvalidated, predicted, and evaluated your frist classifier. Now that you understand the basic steps we will look at a way to combine all these steps together.\n","\n","Before we go further think about what you would have to do if you wanted to change the model. Intalize a new model, change the vairables, redo the cross validation...etc. Seems like a lot. And when we have to change lots of code it is easy to make mistakes. And what if you wanted to try many models and see which one performed best? Or try changing many different features? How could you do it without writing each one out as we have?\n","\n","The solution is to use SKLearn's pipeline class. A pipeline is an object that will execute the various steps in the machine learning process. We can choose what elements we want in the pipeline and those that we do not. Once setup, we can rapidly change models, or input data and have it return our results in an ordered way.\n","\n","\n","1. Initalize a scaler and a classifer object like we did previously.\n","2. Use the ```make_pipeline()``` function to construct a transofmraiton pipeline for the scaler and the classifier\n","3. Input the pipeline object to the cross_validator and evaluate with 5 folds.\n","4. Print out your results (hint: make a function for repetitve tasks like printing)"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c911b68feeb3cc9856690815db075f50","grade":false,"grade_id":"cell-82685348a4b1d16a","locked":false,"schema_version":3,"solution":true,"task":false},"id":"jhwyKe9sTHQ6","outputId":"75a02f46-5211-4173-88fe-5209829cdb9e"},"source":["#define the scaler\n","#define the classifier\n","#make the pipeline\n","#run the cross validation\n","#print results\n","scaler = 0\n","classifier = 0\n","pipe = 0\n","scores = 0\n","# YOUR CODE HERE\n","X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=30, stratify=Y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[6.1 2.9]\n"," [6.7 2.5]\n"," [5.4 3.4]\n"," [6.5 3. ]\n"," [5.5 2.5]\n"," [5.2 3.4]\n"," [5.9 3.2]\n"," [6.  3. ]\n"," [5.1 3.8]\n"," [6.1 3. ]\n"," [7.6 3. ]\n"," [4.4 3. ]\n"," [6.1 2.8]\n"," [5.4 3.9]\n"," [6.9 3.2]\n"," [6.8 3.2]\n"," [6.6 3. ]\n"," [6.6 2.9]\n"," [5.4 3.4]\n"," [7.7 2.6]\n"," [5.2 2.7]\n"," [6.4 2.7]\n"," [6.3 2.5]\n"," [5.  2.3]\n"," [5.1 3.8]\n"," [5.8 2.7]\n"," [4.9 3.1]\n"," [5.8 2.7]\n"," [7.7 3. ]\n"," [6.5 3. ]\n"," [6.4 3.1]\n"," [4.9 3.1]\n"," [7.7 3.8]\n"," [5.6 3. ]\n"," [6.7 3.3]\n"," [5.  3.2]\n"," [6.  2.9]\n"," [5.5 2.3]\n"," [5.5 2.6]\n"," [5.  3. ]\n"," [4.9 3. ]\n"," [5.7 4.4]\n"," [6.7 3.1]\n"," [4.3 3. ]\n"," [5.2 4.1]\n"," [5.6 3. ]\n"," [5.1 2.5]\n"," [6.9 3.1]\n"," [5.1 3.8]\n"," [6.7 3.1]\n"," [6.4 3.2]\n"," [4.8 3.1]\n"," [6.5 2.8]\n"," [6.1 2.8]\n"," [5.5 3.5]\n"," [5.6 2.7]\n"," [7.1 3. ]\n"," [7.  3.2]\n"," [6.4 2.8]\n"," [4.8 3.4]\n"," [5.9 3. ]\n"," [5.1 3.5]\n"," [7.9 3.8]\n"," [6.3 2.3]\n"," [5.8 4. ]\n"," [6.3 2.7]\n"," [5.1 3.7]\n"," [6.2 3.4]\n"," [4.4 3.2]\n"," [6.2 2.2]\n"," [5.7 2.9]\n"," [5.9 3. ]\n"," [4.9 2.4]\n"," [5.7 3. ]\n"," [6.3 3.4]\n"," [6.  2.2]\n"," [4.6 3.4]\n"," [6.5 3.2]\n"," [6.7 3. ]\n"," [5.6 2.8]\n"," [5.2 3.5]\n"," [6.2 2.8]\n"," [5.3 3.7]\n"," [6.5 3. ]\n"," [5.  3.6]\n"," [6.4 2.9]\n"," [6.  3.4]\n"," [7.2 3.6]\n"," [4.6 3.6]\n"," [5.5 2.4]\n"," [7.7 2.8]\n"," [6.3 2.5]\n"," [5.4 3.9]\n"," [5.  3.4]\n"," [5.7 3.8]\n"," [5.8 2.6]\n"," [6.9 3.1]\n"," [6.  2.7]\n"," [5.  2. ]\n"," [6.3 2.9]\n"," [5.8 2.7]\n"," [6.7 3.1]\n"," [5.4 3.7]\n"," [6.8 3. ]\n"," [5.5 4.2]\n"," [6.3 2.8]\n"," [4.7 3.2]\n"," [6.3 3.3]\n"," [4.4 2.9]\n"," [4.8 3.4]\n"," [6.3 3.3]\n"," [6.9 3.1]\n"," [4.5 2.3]\n"," [6.4 2.8]\n"," [5.1 3.4]\n"," [4.9 2.5]\n"," [5.  3.5]\n"," [5.8 2.7]\n"," [5.  3.5]\n"," [5.7 2.8]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b5c8f91e5b08221dee1527dffc0b2a25","grade":true,"grade_id":"cell-0f5c78a78336a3c0","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"id":"iFgSa0alTHQ7"},"source":["assert type(pipe) == type(make_pipeline(scaler, classifier))\n","assert len(cv['test_score']) == 5\n","assert max(cv['test_score']) >= 0.83\n","assert min(cv['test_score']) >= 0.69\n","assert cv['test_score'].mean() >= 0.74"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-h1cH4BTHQ8"},"source":["## Challenge Exercise\n","\n","In this notebook we only used two features to predict the class of the flower. We also did not do any hypter parameter tuning. The challenge is to impove the prediction results. Some ideas we can try:\n","1. Add features to the input and run the cross validation pipeline\n","2. Investigate how to use ```GridSearchCV```, a powerful funtion that searches through hyperparmetrs and does cross validation.\n","    - Hint: Input the pipeline directly into GridSearchCV\n","3. Try a different models like RandomForest or SVM."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"d9c5ac9e443f032b6d2e99fcef3fa7c3","grade":false,"grade_id":"cell-f1ba9880447e80ec","locked":false,"schema_version":3,"solution":true,"task":false},"id":"1DtkI9sPTHQ8"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"690e0a731464fb3ff622b9f4bf584a2a","grade":false,"grade_id":"cell-71c06d508e0294b0","locked":false,"schema_version":3,"solution":true,"task":false},"id":"m1xFPRd9THQ8"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"00a800930107c91e913f6a2e94b3f9eb","grade":false,"grade_id":"cell-230a4548c4b43eb7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"ufi-qUW1THQ9"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"532f0558f08bb8508d769ed40f9caea7","grade":false,"grade_id":"cell-be4475eea0463f76","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Drwb9GB5THQ9"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"59d36e612fde5b4bb9059120c948dee4","grade":false,"grade_id":"cell-b31a03b36b3beb45","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Vx_Sap4-THQ-"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]}]}