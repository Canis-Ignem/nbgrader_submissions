{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005587,
     "end_time": "2020-10-01T00:26:06.206883",
     "exception": false,
     "start_time": "2020-10-01T00:26:06.201296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports\n",
    "\n",
    "The first thing we'll need to do is load in the libraries we'll be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:26:06.225254Z",
     "iopub.status.busy": "2020-10-01T00:26:06.224597Z",
     "iopub.status.idle": "2020-10-01T00:26:07.257012Z",
     "shell.execute_reply": "2020-10-01T00:26:07.256272Z"
    },
    "papermill": {
     "duration": 1.044,
     "end_time": "2020-10-01T00:26:07.257136",
     "exception": false,
     "start_time": "2020-10-01T00:26:06.213136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "#sb.set_theme(style=\"darkgrid\")\n",
    "#np.random.seed(0) # set seed for reproducibility\n",
    "\n",
    "print(\"Numpy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Seaborn:\", sb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10000\n",
    "BINS      = 30\n",
    "npr       = np.random.RandomState(304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7762def48ac13f9e9157580b2d78e26",
     "grade": false,
     "grade_id": "cell-73b2184af646ab0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#2 points \n",
    "# create a dataframe that contains a columns for all the next normalization and scaling methods\n",
    "# Exponential\n",
    "# Lognormal\n",
    "# Chi-sqaured\n",
    "# Weibull\n",
    "# Gaussian\n",
    "# Uniform\n",
    "# Bimodal\n",
    "\n",
    "############### DELETE THE RAISE NOTIMPLEMENTED ERROR LINES!!!!!!\n",
    "\n",
    "df = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7727a8685397b2012859a29d30fb40f",
     "grade": true,
     "grade_id": "cell-ae7968d6618817d8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df[\"Exponential\"].shape[0] == 10000\n",
    "assert df[\"Lognormal\"].shape[0] == 10000\n",
    "assert df[\"Chi-squared\"].shape[0] == 10000\n",
    "assert df[\"Weibull\"].shape[0] == 10000\n",
    "assert df[\"Gaussian\"].shape[0] == 10000\n",
    "assert df[\"Uniform\"].shape[0] == 10000\n",
    "assert df[\"Bimodal\"].shape[0] == 10000\n",
    "assert df.shape == (10000, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=BINS, layout=(3, 4), figsize=(16,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005705,
     "end_time": "2020-10-01T00:26:07.269087",
     "exception": false,
     "start_time": "2020-10-01T00:26:07.263382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scaling vs. Normalization: What's the difference?\n",
    "\n",
    "One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\n",
    "- in **scaling**, you're changing the *range* of your data, while \n",
    "- in **normalization**, you're changing the *shape of the distribution* of your data. \n",
    "\n",
    "Let's talk a little more in-depth about each of these options. \n",
    "\n",
    "# Scaling\n",
    "\n",
    "This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points are, like [support vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of \"1\" in any numeric feature is given the same importance. \n",
    "\n",
    "For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n",
    "\n",
    "By scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6d4ac967495f1994111d364cdc5392c",
     "grade": false,
     "grade_id": "cell-7b06b3bfbc458f9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "# use the min max scaler to scale your your data\n",
    "original_data = df[[\"Exponential\"]]\n",
    "\n",
    "scaled_data = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# plot both together to compare\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "sb.histplot(original_data, ax=ax[0]), ax[0].set_title(\"Original Data\")\n",
    "sb.histplot(scaled_data,   ax=ax[1]), ax[1].set_title(\"Scaled data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bc61fe74b959131b1bb716cd6c5bbfd",
     "grade": true,
     "grade_id": "cell-cf85d2040d8c1d8e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert scaled_data[scaled_data.argmax()] <= 1\n",
    "assert scaled_data[scaled_data.argmin()] >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007431,
     "end_time": "2020-10-01T00:26:07.767278",
     "exception": false,
     "start_time": "2020-10-01T00:26:07.759847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n",
    "\n",
    "# Normalization\n",
    "\n",
    "Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "In general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "The method we're using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2020-10-01T00:26:07.785277Z",
     "iopub.status.busy": "2020-10-01T00:26:07.784655Z",
     "iopub.status.idle": "2020-10-01T00:26:08.165219Z",
     "shell.execute_reply": "2020-10-01T00:26:08.165758Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab694aec5c76af0545b0bc8c0e4972e1",
     "grade": false,
     "grade_id": "cell-94671ef831c6a242",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "papermill": {
     "duration": 0.391053,
     "end_time": "2020-10-01T00:26:08.165898",
     "exception": false,
     "start_time": "2020-10-01T00:26:07.774845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "\n",
    "# normalize the exponential data with boxcox\n",
    "\n",
    "normalized_data = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "sb.histplot(original_data,    ax=ax[0]), ax[0].set_title(\"Original Data\")\n",
    "sb.histplot(normalized_data,  ax=ax[1]), ax[1].set_title(\"Normalized data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b642edd183775720da735c325c7af24",
     "grade": true,
     "grade_id": "cell-68159b6d3588d1c8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert normalized_data[normalized_data.argmax()] < 10\n",
    "assert normalized_data[normalized_data.argmin()] > -7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence \"bell curve\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "247d9f9f3349f0339e6f16baa6a99df7",
     "grade": false,
     "grade_id": "cell-fd73d602ff0531f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "# Get the skewness of all the samples of the original dataframe\n",
    "skew = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81ccf4fb4dc94f4ccb7e6cb84d204320",
     "grade": true,
     "grade_id": "cell-3cbcbdf2330141f3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert skew[0] > 1\n",
    "assert skew[1] > 3\n",
    "assert skew[2] > 1\n",
    "assert -1 < skew[3] < 1\n",
    "assert -1 < skew[4] < 1\n",
    "assert -1 < skew[5] < 1\n",
    "assert -1 < skew[6] < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.allaboutcircuits.com/uploads/articles/understanding-the-normal-distribution-parametric-tests-skewness-and-kurtosis-rk-aac-image2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b00cac518681f8070b610bf066c7ce4",
     "grade": false,
     "grade_id": "cell-f8b7d87f64909213",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import erfinv\n",
    "# 5 points\n",
    "# implement the rank gaussian fucntion from the slides\n",
    "def rank_gauss(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "471f3e4a81af9844bed1813f67327e57",
     "grade": true,
     "grade_id": "cell-362e7a1105baf21d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gauss = rank_gauss(df.Uniform)\n",
    "assert gauss.shape[0] == 10000\n",
    "assert gauss[gauss.argmax()] <= 4\n",
    "assert gauss[gauss.argmin()] >= -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12283f073232403d8eb9483388c7aee8",
     "grade": false,
     "grade_id": "cell-15b6c59332351df4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "# create a dictionary with the following enconders\n",
    "'''\n",
    "encoders = {\n",
    "  # Scalers\n",
    "  \"Min-max scaling\":   \n",
    "  \"Max-abs scaling\":    \n",
    "  \"Standard scaling\":   \n",
    "  \"Robust scaling\":     \n",
    "    \n",
    "  # Normalizers\n",
    "  \"Box-Cox\":                # Can only be used for positive values\n",
    "  \"Yeo-Johnson\":         # Similar to Box-cox but can be used for negative values.\n",
    "  \"Quantile (normal)\":  \n",
    "  \"Quantile (uniform)\": \n",
    "}\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9019d89bc0508e91ff93188f1d1c21e0",
     "grade": true,
     "grade_id": "cell-bd44d6b1c55ae2c1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(encoders[\"Min-max scaling\"]) is sklearn.preprocessing._data.MinMaxScaler\n",
    "assert type(encoders[\"Max-abs scaling\"]) is sklearn.preprocessing._data.MaxAbsScaler\n",
    "assert type(encoders[\"Standard scaling\"]) is sklearn.preprocessing._data.StandardScaler\n",
    "assert type(encoders[\"Robust scaling\"]) is sklearn.preprocessing._data.RobustScaler\n",
    "\n",
    "assert type(encoders[\"Box-Cox\"]) is sklearn.preprocessing._data.PowerTransformer\n",
    "assert type(encoders[\"Yeo-Johnson\"]) is sklearn.preprocessing._data.PowerTransformer\n",
    "assert type(encoders[\"Quantile (normal)\"]) is sklearn.preprocessing._data.QuantileTransformer\n",
    "assert type(encoders[\"Quantile (uniform)\"]) is sklearn.preprocessing._data.QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original distributions\")\n",
    "df.hist(bins=BINS, layout=(1,7), figsize=(16,2))\n",
    "plt.show()\n",
    "for encoder_name, encoder in encoders.items():\n",
    "    print(encoder_name)\n",
    "    df_tmp = pd.DataFrame(encoder.fit_transform(df), columns=df.columns)\n",
    "    df_tmp.hist(bins=BINS, layout=(1,7), figsize=(16,2))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008392,
     "end_time": "2020-10-01T00:26:08.183120",
     "exception": false,
     "start_time": "2020-10-01T00:26:08.174728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Your turn\n",
    "\n",
    "It's time to [**apply what you just learned**](https://www.kaggle.com/kernels/fork/10824404) a dataset of Kickstarter projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "duration": 6.671696,
   "end_time": "2020-10-01T00:26:08.316803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-01T00:26:01.645107",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
